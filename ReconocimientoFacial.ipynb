{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento facial\n",
    "\n",
    "En este notebook vamos a construir un sistema de reconocimiento facial. La mayoría de las ideas presentadas aquí provienen de [FaceNet](https://arxiv.org/pdf/1503.03832.pdf). Otra red que se utiliza habitualmente para el reconocimiento facial es [DeepFace](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf). \n",
    "\n",
    "Los problemas de reconocimiento facial se pueden clasificar en dos categorías.\n",
    "\n",
    "- La **verificación facial** intenta responder a la pregunta ¿es esta persona quien dice ser?. Por ejemplo, en algunos aeropuertos (sobre todo en China) es posible pasar algunos controles mediante un sistema que escanéa el pasaporte y verifica que el portador de dicho pasaporte es el propietario del mismo. Una aplicación móvil que se desbloquea usando tu cara también usa verificación facial. Es decir, este es un problema de emparejamiento 1 a 1. \n",
    "- El **reconocimiento facial** intenta responder a la pregunta ¿quién es esta persona? Por ejemplo, la empresa [Baidu](https://www.youtube.com/watch?v=wr4rx0Spihs) permite a sus empleados entrar a la oficina sin necesidad de identificarse. Esto es un problema de emparejamiento 1 a K. \n",
    "\n",
    "FaceNet es una red neuronal que aprende a codificar la imagen de una cara en un vector de 128 números. Al comparar dos de esos vectores se puede determinar si dos imágenes son de la misma persona. \n",
    "\n",
    "En este notebook aprenderás:\n",
    "- La definición de la función de pérdida triple.\n",
    "- Cómo usar un modelo entrenado para asignar imágenes de caras a una codificación en forma de un vector de tamaño 128.\n",
    "- Cómo usar esas codificaciones para llevar a cabo verificación facial y reconocimiento facial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.  Carga de paquetes\n",
    "Comenzamos cargando los paquetes necesarios. Posiblemente necesites instalar alguno de ellos utilizando `pip`. El paquete más difícil de instalar es OpenCV, y las instrucciones para la instalación de dicho paquete las puedes encontrar en el aula virtual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Verificación facial ingenua\n",
    "\n",
    "En el problema de la verificación facial se proporcionan dos imágenes y el programa tiene que decidir si pertenecen a la misma persona. La manera más sencilla de hacer esto es comparar las dos imágenes pixel a pixel. Si la distancia entre las dos imágenes es menor que un determinado umbral, entonces las dos imágenes pertenecen a la misma persona.\n",
    "\n",
    "<img src=\"images/pixel_comparison.png\" style=\"width:380px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 1** </u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Como puedes imaginar, este algoritmo no funciona demasiado bien, dado que los valores de píxeles cambian de forma dramática dependiendo de las condiciones de luz, orientación de la cara de la persona, pequeños cambios en la inclinación de la cabeza, etc. \n",
    "\n",
    "Por lo tanto, en lugar de usar directamente la imagen, es mejor utilizar una codificación $f(img)$ de manera que la comparación de imágenes representadas mediante esa codificación permita decidir si las dos imágenes pertenecen a la misma persona.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Codificando las imágenes faciales usando un vector de 128 componentes\n",
    "\n",
    "### 2.1 - Usando una red para calcular las codificaciones\n",
    "\n",
    "El modelo FaceNet utiliza una gran cantidad de datos y lleva bastante tiempo entrenarlo. Por lo que una solución que se suele aplicar normalmente consiste en cargar los pesos de una red que ya ha sido entrenada. La red que utilizamos en este caso es el modelo Inception  [Szegedy *et al.*](https://arxiv.org/abs/1409.4842). Esta red está disponible en el fichero `inception_blocks.py` pero no es necesario entrar en detalle de cómo funciona esta red para este notebook. El modelo Inception fue entrenado para clasificar imágenes en 1000 categorías, para ser capaces de producir una codificación de 128 componentes lo que se hace es eliminar la última capa de la red y utilizar la penúltima capa (que tiene 128 nodos) como salida. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los aspectos clave a tener en cuenta son:\n",
    "- La red usa imágenes RGB de tamaño 96x96 como entrada. Concretamente toma una imagen de una cara (o lote de $m$ imágenes) con la forma $(m, 3, 96, 96)$ (si hay solo una imagen $m=1$).\n",
    "- La red produce como salida una matriz de la forma $(m, 128)$ que codifica cada una de las imágenes que ha recibido como entrada en un vector de 128 componentes. \n",
    "\n",
    "Ejecuta la siguiente celda para crear el modelo de las caras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Parametros totales de la red:', 3743280)\n"
     ]
    }
   ],
   "source": [
    "print(\"Parametros totales de la red:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura de la red\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 3, 96, 96)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_93 (ZeroPadding2D (None, 3, 102, 102)   0           input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 64, 48, 48)    9472        zero_padding2d_93[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)         (None, 64, 48, 48)    256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_149 (Activation)      (None, 64, 48, 48)    0           bn1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_94 (ZeroPadding2D (None, 64, 50, 50)    0           activation_149[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D)  (None, 64, 24, 24)    0           zero_padding2d_94[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                   (None, 64, 24, 24)    4160        max_pooling2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bn2 (BatchNormalization)         (None, 64, 24, 24)    256         conv2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_150 (Activation)      (None, 64, 24, 24)    0           bn2[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_95 (ZeroPadding2D (None, 64, 26, 26)    0           activation_150[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                   (None, 192, 24, 24)   110784      zero_padding2d_95[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bn3 (BatchNormalization)         (None, 192, 24, 24)   768         conv3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_151 (Activation)      (None, 192, 24, 24)   0           bn3[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_96 (ZeroPadding2D (None, 192, 26, 26)   0           activation_151[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D)  (None, 192, 12, 12)   0           zero_padding2d_96[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_3x3_conv1 (Conv2D)  (None, 96, 12, 12)    18528       max_pooling2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_5x5_conv1 (Conv2D)  (None, 16, 12, 12)    3088        max_pooling2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_3x3_bn1 (BatchNorma (None, 96, 12, 12)    384         inception_3a_3x3_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_5x5_bn1 (BatchNorma (None, 16, 12, 12)    64          inception_3a_5x5_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_152 (Activation)      (None, 96, 12, 12)    0           inception_3a_3x3_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_154 (Activation)      (None, 16, 12, 12)    0           inception_3a_5x5_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D)  (None, 192, 5, 5)     0           max_pooling2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_97 (ZeroPadding2D (None, 96, 14, 14)    0           activation_152[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_98 (ZeroPadding2D (None, 16, 16, 16)    0           activation_154[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_pool_conv (Conv2D)  (None, 32, 5, 5)      6176        max_pooling2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_3x3_conv2 (Conv2D)  (None, 128, 12, 12)   110720      zero_padding2d_97[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_5x5_conv2 (Conv2D)  (None, 32, 12, 12)    12832       zero_padding2d_98[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_pool_bn (BatchNorma (None, 32, 5, 5)      128         inception_3a_pool_conv[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_1x1_conv (Conv2D)   (None, 64, 12, 12)    12352       max_pooling2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_3x3_bn2 (BatchNorma (None, 128, 12, 12)   512         inception_3a_3x3_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_5x5_bn2 (BatchNorma (None, 32, 12, 12)    128         inception_3a_5x5_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_156 (Activation)      (None, 32, 5, 5)      0           inception_3a_pool_bn[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_3a_1x1_bn (BatchNormal (None, 64, 12, 12)    256         inception_3a_1x1_conv[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_153 (Activation)      (None, 128, 12, 12)   0           inception_3a_3x3_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_155 (Activation)      (None, 32, 12, 12)    0           inception_3a_5x5_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_99 (ZeroPadding2D (None, 32, 12, 12)    0           activation_156[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_157 (Activation)      (None, 64, 12, 12)    0           inception_3a_1x1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)     (None, 256, 12, 12)   0           activation_153[0][0]             \n",
      "                                                                   activation_155[0][0]             \n",
      "                                                                   zero_padding2d_99[0][0]          \n",
      "                                                                   activation_157[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_3x3_conv1 (Conv2D)  (None, 96, 12, 12)    24672       concatenate_29[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_5x5_conv1 (Conv2D)  (None, 32, 12, 12)    8224        concatenate_29[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_3x3_bn1 (BatchNorma (None, 96, 12, 12)    384         inception_3b_3x3_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_5x5_bn1 (BatchNorma (None, 32, 12, 12)    128         inception_3b_5x5_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_158 (Activation)      (None, 96, 12, 12)    0           inception_3b_3x3_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_160 (Activation)      (None, 32, 12, 12)    0           inception_3b_5x5_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePoo (None, 256, 4, 4)     0           concatenate_29[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_100 (ZeroPadding2 (None, 96, 14, 14)    0           activation_158[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_101 (ZeroPadding2 (None, 32, 16, 16)    0           activation_160[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_pool_conv (Conv2D)  (None, 64, 4, 4)      16448       average_pooling2d_17[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_3x3_conv2 (Conv2D)  (None, 128, 12, 12)   110720      zero_padding2d_100[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_5x5_conv2 (Conv2D)  (None, 64, 12, 12)    51264       zero_padding2d_101[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_pool_bn (BatchNorma (None, 64, 4, 4)      256         inception_3b_pool_conv[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_1x1_conv (Conv2D)   (None, 64, 12, 12)    16448       concatenate_29[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_3x3_bn2 (BatchNorma (None, 128, 12, 12)   512         inception_3b_3x3_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_5x5_bn2 (BatchNorma (None, 64, 12, 12)    256         inception_3b_5x5_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_162 (Activation)      (None, 64, 4, 4)      0           inception_3b_pool_bn[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_3b_1x1_bn (BatchNormal (None, 64, 12, 12)    256         inception_3b_1x1_conv[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_159 (Activation)      (None, 128, 12, 12)   0           inception_3b_3x3_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_161 (Activation)      (None, 64, 12, 12)    0           inception_3b_5x5_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_102 (ZeroPadding2 (None, 64, 12, 12)    0           activation_162[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_163 (Activation)      (None, 64, 12, 12)    0           inception_3b_1x1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)     (None, 320, 12, 12)   0           activation_159[0][0]             \n",
      "                                                                   activation_161[0][0]             \n",
      "                                                                   zero_padding2d_102[0][0]         \n",
      "                                                                   activation_163[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_3x3_conv1 (Conv2D)  (None, 128, 12, 12)   41088       concatenate_30[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_5x5_conv1 (Conv2D)  (None, 32, 12, 12)    10272       concatenate_30[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_3x3_bn1 (BatchNorma (None, 128, 12, 12)   512         inception_3c_3x3_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_5x5_bn1 (BatchNorma (None, 32, 12, 12)    128         inception_3c_5x5_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_164 (Activation)      (None, 128, 12, 12)   0           inception_3c_3x3_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_166 (Activation)      (None, 32, 12, 12)    0           inception_3c_5x5_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_103 (ZeroPadding2 (None, 128, 14, 14)   0           activation_164[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_104 (ZeroPadding2 (None, 32, 16, 16)    0           activation_166[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_3x3_conv2 (Conv2D)  (None, 256, 6, 6)     295168      zero_padding2d_103[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_5x5_conv2 (Conv2D)  (None, 64, 6, 6)      51264       zero_padding2d_104[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_3x3_bn2 (BatchNorma (None, 256, 6, 6)     1024        inception_3c_3x3_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_3c_5x5_bn2 (BatchNorma (None, 64, 6, 6)      256         inception_3c_5x5_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D)  (None, 320, 5, 5)     0           concatenate_30[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_165 (Activation)      (None, 256, 6, 6)     0           inception_3c_3x3_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_167 (Activation)      (None, 64, 6, 6)      0           inception_3c_5x5_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_105 (ZeroPadding2 (None, 320, 6, 6)     0           max_pooling2d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)     (None, 640, 6, 6)     0           activation_165[0][0]             \n",
      "                                                                   activation_167[0][0]             \n",
      "                                                                   zero_padding2d_105[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_3x3_conv1 (Conv2D)  (None, 96, 6, 6)      61536       concatenate_31[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_5x5_conv1 (Conv2D)  (None, 32, 6, 6)      20512       concatenate_31[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_3x3_bn1 (BatchNorma (None, 96, 6, 6)      384         inception_4a_3x3_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_5x5_bn1 (BatchNorma (None, 32, 6, 6)      128         inception_4a_5x5_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_168 (Activation)      (None, 96, 6, 6)      0           inception_4a_3x3_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_170 (Activation)      (None, 32, 6, 6)      0           inception_4a_5x5_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePoo (None, 640, 2, 2)     0           concatenate_31[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_106 (ZeroPadding2 (None, 96, 8, 8)      0           activation_168[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_107 (ZeroPadding2 (None, 32, 10, 10)    0           activation_170[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_pool_conv (Conv2D)  (None, 128, 2, 2)     82048       average_pooling2d_18[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_3x3_conv2 (Conv2D)  (None, 192, 6, 6)     166080      zero_padding2d_106[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_5x5_conv2 (Conv2D)  (None, 64, 6, 6)      51264       zero_padding2d_107[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_pool_bn (BatchNorma (None, 128, 2, 2)     512         inception_4a_pool_conv[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_1x1_conv (Conv2D)   (None, 256, 6, 6)     164096      concatenate_31[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_3x3_bn2 (BatchNorma (None, 192, 6, 6)     768         inception_4a_3x3_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_5x5_bn2 (BatchNorma (None, 64, 6, 6)      256         inception_4a_5x5_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_172 (Activation)      (None, 128, 2, 2)     0           inception_4a_pool_bn[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_4a_1x1_bn (BatchNormal (None, 256, 6, 6)     1024        inception_4a_1x1_conv[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_169 (Activation)      (None, 192, 6, 6)     0           inception_4a_3x3_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_171 (Activation)      (None, 64, 6, 6)      0           inception_4a_5x5_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_108 (ZeroPadding2 (None, 128, 6, 6)     0           activation_172[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_173 (Activation)      (None, 256, 6, 6)     0           inception_4a_1x1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)     (None, 640, 6, 6)     0           activation_169[0][0]             \n",
      "                                                                   activation_171[0][0]             \n",
      "                                                                   zero_padding2d_108[0][0]         \n",
      "                                                                   activation_173[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_3x3_conv1 (Conv2D)  (None, 160, 6, 6)     102560      concatenate_32[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_5x5_conv1 (Conv2D)  (None, 64, 6, 6)      41024       concatenate_32[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_3x3_bn1 (BatchNorma (None, 160, 6, 6)     640         inception_4e_3x3_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_5x5_bn1 (BatchNorma (None, 64, 6, 6)      256         inception_4e_5x5_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_174 (Activation)      (None, 160, 6, 6)     0           inception_4e_3x3_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_176 (Activation)      (None, 64, 6, 6)      0           inception_4e_5x5_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_109 (ZeroPadding2 (None, 160, 8, 8)     0           activation_174[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_110 (ZeroPadding2 (None, 64, 10, 10)    0           activation_176[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_3x3_conv2 (Conv2D)  (None, 256, 3, 3)     368896      zero_padding2d_109[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_5x5_conv2 (Conv2D)  (None, 128, 3, 3)     204928      zero_padding2d_110[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_3x3_bn2 (BatchNorma (None, 256, 3, 3)     1024        inception_4e_3x3_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_4e_5x5_bn2 (BatchNorma (None, 128, 3, 3)     512         inception_4e_5x5_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D)  (None, 640, 2, 2)     0           concatenate_32[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_175 (Activation)      (None, 256, 3, 3)     0           inception_4e_3x3_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_177 (Activation)      (None, 128, 3, 3)     0           inception_4e_5x5_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_111 (ZeroPadding2 (None, 640, 3, 3)     0           max_pooling2d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)     (None, 1024, 3, 3)    0           activation_175[0][0]             \n",
      "                                                                   activation_177[0][0]             \n",
      "                                                                   zero_padding2d_111[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_3x3_conv1 (Conv2D)  (None, 96, 3, 3)      98400       concatenate_33[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_3x3_bn1 (BatchNorma (None, 96, 3, 3)      384         inception_5a_3x3_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_178 (Activation)      (None, 96, 3, 3)      0           inception_5a_3x3_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePoo (None, 1024, 1, 1)    0           concatenate_33[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_112 (ZeroPadding2 (None, 96, 5, 5)      0           activation_178[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_pool_conv (Conv2D)  (None, 96, 1, 1)      98400       average_pooling2d_19[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_3x3_conv2 (Conv2D)  (None, 384, 3, 3)     332160      zero_padding2d_112[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_pool_bn (BatchNorma (None, 96, 1, 1)      384         inception_5a_pool_conv[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_1x1_conv (Conv2D)   (None, 256, 3, 3)     262400      concatenate_33[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_3x3_bn2 (BatchNorma (None, 384, 3, 3)     1536        inception_5a_3x3_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_180 (Activation)      (None, 96, 1, 1)      0           inception_5a_pool_bn[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_5a_1x1_bn (BatchNormal (None, 256, 3, 3)     1024        inception_5a_1x1_conv[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_179 (Activation)      (None, 384, 3, 3)     0           inception_5a_3x3_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_113 (ZeroPadding2 (None, 96, 3, 3)      0           activation_180[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_181 (Activation)      (None, 256, 3, 3)     0           inception_5a_1x1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)     (None, 736, 3, 3)     0           activation_179[0][0]             \n",
      "                                                                   zero_padding2d_113[0][0]         \n",
      "                                                                   activation_181[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_3x3_conv1 (Conv2D)  (None, 96, 3, 3)      70752       concatenate_34[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_3x3_bn1 (BatchNorma (None, 96, 3, 3)      384         inception_5b_3x3_conv1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_182 (Activation)      (None, 96, 3, 3)      0           inception_5b_3x3_bn1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling2D)  (None, 736, 1, 1)     0           concatenate_34[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_114 (ZeroPadding2 (None, 96, 5, 5)      0           activation_182[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_pool_conv (Conv2D)  (None, 96, 1, 1)      70752       max_pooling2d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_3x3_conv2 (Conv2D)  (None, 384, 3, 3)     332160      zero_padding2d_114[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_pool_bn (BatchNorma (None, 96, 1, 1)      384         inception_5b_pool_conv[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_1x1_conv (Conv2D)   (None, 256, 3, 3)     188672      concatenate_34[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_3x3_bn2 (BatchNorma (None, 384, 3, 3)     1536        inception_5b_3x3_conv2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_184 (Activation)      (None, 96, 1, 1)      0           inception_5b_pool_bn[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "inception_5b_1x1_bn (BatchNormal (None, 256, 3, 3)     1024        inception_5b_1x1_conv[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_183 (Activation)      (None, 384, 3, 3)     0           inception_5b_3x3_bn2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_115 (ZeroPadding2 (None, 96, 3, 3)      0           activation_184[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_185 (Activation)      (None, 256, 3, 3)     0           inception_5b_1x1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)     (None, 736, 3, 3)     0           activation_183[0][0]             \n",
      "                                                                   zero_padding2d_115[0][0]         \n",
      "                                                                   activation_185[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePoo (None, 736, 1, 1)     0           concatenate_35[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 736)           0           average_pooling2d_20[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_layer (Dense)              (None, 128)           94336       flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, 128)           0           dense_layer[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,743,280\n",
      "Trainable params: 3,733,968\n",
      "Non-trainable params: 9,312\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Estructura de la red\")\n",
    "FRmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar la salida de la red para comparar dos imágenes de la siguiente manera.\n",
    "\n",
    "<img src=\"images/distance_kiank.png\" style=\"width:680px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 2**: <br> </u> <font color='purple'>Calculando la distancia entre las dos codificaciones y usando un umbral se puede determinar si las dos imágenes representan a la misma persona.</center></caption>\n",
    "\n",
    "Así que una buena codificación es aquella en la cual:\n",
    "- Si dos imágenes pertenecen a la misma persona, las codificaciones de dichas imágenes serán similares. \n",
    "- Si dos imágenes pertenecen a personas distintas, las codificaciones de dichas imágenes serán diferentes.\n",
    "\n",
    "La función que se suele utilizar para medir esta distancia es la pérdida triple, la cual se intenta acercar las codificaciones de las dos imágenes de una misma persona (imagen ancla e imagen positiva), mientras que intenta alejar las codificaciones de dos personas distintas (imagen ancla e imagen negativa).\n",
    "\n",
    "<img src=\"images/triplet_comparison.png\" style=\"width:280px;height:150px;\">\n",
    "<br>\n",
    "<caption><center> <u> <font color='purple'> **Figura 3**: <br> </u> <font color='purple'>De izquierda a derecha: imagen Ancla (A), imagen Positiva (P), imagen Negativa (N) </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.2 - La pérdida triple\n",
    "\n",
    "Para una imagen $x$, denotaremos a su codificación como $f(x)$ donde $f$ es la función construida por la red neuronal. \n",
    "\n",
    "<img src=\"images/f_x.png\" style=\"width:380px;height:150px;\">\n",
    "\n",
    "<!--\n",
    "We will also add a normalization step at the end of our model so that $\\mid \\mid f(x) \\mid \\mid_2 = 1$ (means the vector of encoding should be of norm 1).\n",
    "!-->\n",
    "\n",
    "Para el entrenamiento usaremos tríos de imágenes $(A, P, N)$:  \n",
    "\n",
    "- A es la imagen \"Ancla\": una imagen de una persona. \n",
    "- P es una imagen \"Positiva\": una imagen de la misma persona representada en la imagen ancla.\n",
    "- N es una imagen \"Negativa\": una imagen de una persona distinta a la de la imagen ancla.\n",
    "\n",
    "Estos tríos se van tomando de nuestro conjunto de entrenamiento. Escribiremos $(A^{(i)}, P^{(i)}, N^{(i)})$ para denotar el $i$-ésimo ejemplo de entrenamiento.\n",
    "\n",
    "Nos queremos asegurar que una  imagen $A^{(i)}$ sea más cercana a la imagen Positiva $P^{(i)}$ que a la imagen Negativa $N^{(i)}$) por al menos un margen $\\alpha$. Es decir:\n",
    "\n",
    "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
    "\n",
    "Ahora queremos minimizar esta función de coste triple:\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{N}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}$$\n",
    "\n",
    "Aquí usamos la notación \"$[z]_+$\" para denotar $max(z,0)$.  \n",
    "\n",
    "Notas:\n",
    "- El término (1) es la distancia al cuadrado entre la imagen ancla y la imagen positiva para un determinado trío, queremos que esta distancia sea pequeña.\n",
    "- El término (2) es la distancia al cuadrado entre la imagen ancla y la imagen negativa para un determinado trío, queremos que esta distancia sea relativamente grande\n",
    "- $\\alpha$ se llama el margen y es un hiperparámetro que hay que elegir manualmente. En nuestro caso usamos $\\alpha = 0.2$. \n",
    "\n",
    "La pérdida triple se implementa a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Cargando un modelo pre-entrenado\n",
    "\n",
    "FaceNet está entrenado para minimizar la función de pérdida triple. Pero debido a que este proceso de entrenamiento lleva mucho tiempo, nosotros no lo entrenaremos desde cero. En su lugar, cargamos un modelo previamente entrenado. Para ello debes ejecutar la siguiente celda, lo cual puede llevar algunos minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran algunos ejemplos de las distancias entre las codificaciones de tres personas.\n",
    "<img src=\"images/distance_matrix.png\" style=\"width:380px;height:200px;\">\n",
    "<br>\n",
    "<caption><center> <u> <font color='purple'> **Figura 4**:</u> <br>  <font color='purple'> Ejemplo de las distancias entre las codificaciones de tres personas.</center></caption>\n",
    "\n",
    "Veámos ahora cómo usar este modelo para llevar a cabo verificación facial y reconocimiento facial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Aplicando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos construir un sistema que permita el acceso a nuestra casa usando un sistema de verificación facial. En concreto para admitir a una persona en nuestra casa deberá pasar una tarjeta de identificación para identificarse. El sistema se encargará de verificar si la persona que ha pasado la tarjeta es quien dice ser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Verificación facial\n",
    "\n",
    "Empezamos construyendo una base de datos que contiene la codificación de cada persona que tiene permitido el acceso a nuestra casa. Para generar la codificación se utiliza la función `img_to_encoding(image_path, model)` que básicamente ejecuta el paso de propagación hacia delante del modelo en una imagen.\n",
    "\n",
    "Ejecuta el siguiente código para construir la base de datos que se representa mediante un diccionario Python. Esta base de datos asigna un vector de tamaño 128 a cada cara. Las imágenes que se añaden a la base de datos están en la carpeta `images`, así que puedes añadir tus propias imágenes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de datos tiene como identificador el nombre (una mala idea, pero por simplicidad) lo hacemos así. \n",
    "\n",
    "Ahora cuando alguien se presenta en la entrada de nuestra casa y pasa su tarjeta de identificación (lo cual nos da su nombre) podemos buscar en la base de datos la codificación asociada a su nombre y comprobar si la persona que se encuentra en la puerta coincide con la que está en la tarjeta de identificación. \n",
    "\n",
    "La siguiente función comprueba si la imagen capturada por la puerta de nuestra casa (disponible en `image_path`) pertenece realmente a la persona que dice ser. Para ello:\n",
    "1. Se calcula la codificación de la imagen capturada por la cámara de la casa.\n",
    "2. Se calcula la distancia a la codificación de la imagen de la persona de la tarjeta en la base de datos.\n",
    "3. Se abre la puerta si la distancia es menor que 0.7, en caso contrario la puerta no se abre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n",
    "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "    door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "\n",
    "    # Step 2: Compute distance with identity's image (≈ 1 line)\n",
    "    dist = np.linalg.norm(encoding-database[identity])\n",
    "    \n",
    "    # Step 3: Open the door if dist < 0.7, else don't open (≈ 3 lines)\n",
    "    if dist<0.7:\n",
    "        print(\"Eres \" + str(identity) + \", bienvenido\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"No eres \" + str(identity) + \", por favor márchate\")\n",
    "        door_open = False\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes está intentando entrar a la casa y la cámara toma una imagen de el(\"images/camera_0.jpg\"). Vamos a ejectar nuestro algoritmo de verificación en dicha imagen.\n",
    "\n",
    "<img src=\"images/camera_0.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres younes, bienvenido\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.67291224, True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Benoit no tiene permitido el acceso a nuestra casa, pero le ha robado la tarjeta a Kian y está intentando entrar. La cámara frontal de nuestra casa toma una imagen de Benoit (\"images/camera_2.jpg). Veámos si Benoit puede entrar a nuestra casa.\n",
    "<img src=\"images/camera_2.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No eres kian, por favor márchate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.86543161, False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Reconocimiento facial\n",
    "\n",
    "El sistema de verifiación facial está funcionando bastante bien. Pero debido a que a Kian le han robado su tarjeta, cuando intente volver, no podrá entrar. \n",
    "\n",
    "Para resolver este problema vamos a implantar un sistema de reconocimiento facial. De este modo no será necesario llevar una tarjeta de identificación nunca mas: toda persona autorizada podrá acercarse a nuestra casa, y la puerta de entrada se abrirá. \n",
    "\n",
    "Para ello vamos a implementar un sistema de reconocimiento facial que toma una imagen y comprueba si es de una de las personas autorizadas. Al contrario que en el caso anterior, no necesitamos ninguna información adicional. La siguiente función implementa dicha funcionalidad con los siguientes pasos:\n",
    "1. Calcula la codificación objetivo de la imagen.\n",
    "2. Encuentra la codificación en la base de datos que es más cercana a la codificación objetivo. Para ello:\n",
    "    - Inicializa la variable `min_dist` a un número lo suficientemente grande. \n",
    "    - Se itera sobre el diccionario de la base de datos:\n",
    "        - Se calcula la distancia L2 entre la codificación objetivo y la codificación actual. \n",
    "        - Si la distancia es menor que min_dist, se asigna dicha distancia a min_dist y se identifica por nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    \n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n",
    "        dist = np.linalg.norm(encoding-database[name])\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
    "        if dist<min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    if min_dist > 0.7:\n",
    "        print(\"No esta en la base de datos.\")\n",
    "    else:\n",
    "        print (\"Eres \" + str(identity) + \", la distancia es \" + str(min_dist) + \", bienvenido\")\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes se encuentra en la puerta de entrada y la cámara toma una imagen de el. Veámos si el algoritmo es capaz de identificarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres younes, la distancia es 0.672912, bienvenido\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.67291224, 'younes')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos listos nuestros sistemas de verificación y reconocimiento facial. \n",
    "\n",
    "El algoritmo aquí presentado se puede mejorar de muchas maneras:\n",
    "- Poniendo más imágenes de cada persona para incrementar la accuracy del modelo. \n",
    "- Recortar las imágenes para que contengan solo la cara con el objetivo de hacer el algoritmo más robusto.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias:\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
